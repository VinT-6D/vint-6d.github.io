saz<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="description" content="Vint-6D" />
        <meta name="author" content="anonymous" />

        <title>VinT-6D</title>
        <!-- Bootstrap core CSS -->
        <link href="bootstrap.min.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <!-- <link href="offcanvas.css" rel="stylesheet" /> -->
        <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
        <style type="text/css">
            .container {
                zoom: 1;
                margin-left: auto;
                margin-right: auto;
                vertical-align: middle;
                width: 100%;
                max-width: 1000px;
            }
            .container_img {
                zoom: 1;
                margin-left: auto;
                margin-right: auto;
                vertical-align: middle;
                width: 100%;
                max-width: 1000px;
            }
            .message {
                padding: 1rem;
                margin-bottom: 1rem;
                color: #212529;
                background-color: #fff3bf;
                border-radius: 0.25rem;
                text-align: center;
                vertical-align: middle;
                width: 50%;
            }
        </style>
    </head>

    <body>
        <div class="jumbotron jumbotron-fluid">
            <div class="container" align="center">
                <h2>
                    VinT-6D: A Large-Scale Object-in-hand Dataset from Vision, Touch and Proprioception
                </h2>
                <h4><i>International Conference on Machine Learning (ICML) 2024</i></h4>
                <p>
                    <a href="https://wan-zhaoliang.vercel.app/" target="_blank">Zhaoliang Wan</a><sup>1,2</sup>&nbsp;
                    <a href="https://ygling2008.github.io/" target="_blank">Yonggen Ling</a><sup>2</sup>&nbsp;
                    <a href="#" target="_blank">Senlin Yi</a><sup>1</sup>&nbsp;
                    <a href="http://luqi.info/" target="_blank">Lu Qi</a><sup>3</sup>&nbsp;
                    <a href="#" target="_blank">Minglei Lu</a><sup>2</sup>&nbsp;
                    <a href="#" target="_blank">Wangwei Lee</a><sup>2</sup>&nbsp;
                    <a href="#" target="_blank">Sicheng Yang</a><sup>2</sup>&nbsp;
                    <a href="#" target="_blank">Xiao Teng</a><sup>2</sup>&nbsp;
                    <a href="#" target="_blank">Peng Lu</a><sup>2</sup>&nbsp;
                    <a href="https://people.ucas.edu.cn/~XuYang" target="_blank">Xu Yang</a><sup>4</sup>&nbsp;
                    <a href="https://faculty.ucmerced.edu/mhyang/" target="_blank">Ming-Hsuan Yang</a><sup>3</sup>&nbsp;
                    <a href="https://cse.sysu.edu.cn/content/2504" target="_blank">Hui Cheng</a><sup>1</sup>&nbsp;
                    <br/>
                    <sup>1</sup><a href="#" target="_blank">Sun Yat-Sen University</a> &nbsp;&nbsp;
                    <sup>2</sup><a href="#" target="_blank">Tencnet</a> &nbsp;&nbsp;
                    <sup>3</sup><a href="#" target="_blank">UC Merced</a> &nbsp;&nbsp;
                    <sup>4</sup><a href="#" target="_blank">Chinese Academic of Sciences, Automation Institute</a> &nbsp;&nbsp;
                    <br/>
                </p>
                <h4>
                    <a href="#">[Paper]</a>&nbsp;
                    <a href="#">[DataSet]</a>&nbsp;
                    <a href="#">[Video] (Coming Soon)</a>&nbsp;
                    <!-- <a href="#">[Code]</a>&nbsp; -->
                    <!-- <a href="#" style="color: grey;">[arXiv]</a>&nbsp; -->
                    <!-- <a href="#">[Session]</a>&nbsp; -->
                    <!-- <a href="#">[Slides]</a>&nbsp; -->
                    <!-- <a href="#">[Poster]</a>&nbsp; -->
                </h4>
                <h4>
                    <!-- <a href="#" style="color: grey;">[Twitter Thread]</a>&nbsp; -->
                </h4>
            </div>
        </div>

        <div class="section">

            <div class="container_img">
                <p align="center">
                    <img border="0" src="data/imgs/teaser.png" style="width: 100%;"/>
                </p>
            </div>
            <div class="container">
                <div align="center"><h1>Abstract</h1></div>
                <hr />
                <p style="text-align: justify; font-size: medium;">
                    This paper addresses the scarcity of large-scale
                    datasets for accurate object-in-hand pose estimation, which is crucial for robotic in-hand manipu-
                    lation within the “Perception-Planning-Control”
                    paradigm. Specifically, we introduce VinT-6D,
                    the first extensive multi-modal dataset integrating vision, touch, and proprioception, to enhance
                    robotic manipulation. VinT-6D comprises 2 milion VinT-Sim and 0.1 million VinT-Real entries,
                    collected via simulations in Mujoco and Blender
                    and a custom-designed real-world platform. This
                    dataset is tailored for robotic hands, offering models with whole-hand tactile perception and high-quality, well-aligned data. To the best of our
                    knowledge, the VinT-Real is the largest considering the collection difficulties in the real-world environment so it can bridge the gap of simulation to
                    real compared to the previous works. Built upon
                    VinT-6D, we present a benchmark method that
                    shows significant improvements in performance
                    by fusing multi-modal information. The release
                    of the VinT-6D dataset and benchmark code will
                    soon provide a valuable resource for research and
                    development in robotic manipulation.
                </p>

                <div class="container_img">
                    <script
                        type="module"
                        src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js"
                    ></script>

                    <div align="center">
                        <h1>VinT-Sim Pipeline</h1>
                    </div>
                    <hr />

                    <div class="row" id="method" style="max-width:1000px; margin:0 auto; text-align:justify">
                        <center>
                            <img src="./data/imgs/vint_sim_pipeline_1.png" width="100%">
                        </center>
                        <br>
                        <p style="text-align: justify; font-size: medium;">
                            <b>VinT-Sim</b> requires a robotic hand as input, which can have either three or four
                            fingers along with an object model. There are three components
                            involved in this process: (1) Object-grasp interaction is used to
                            generate tactile data and proprioception information. (2) Use different grasps to generate different grasp scenes. (3) Each scene is
                            rendered with different realistic backgrounds and sampled from
                            multiple views.
                        </p>
                        <br>
                    </div>
                    <div align="center">
                        <h1>VinT-Real Collection</h1>
                    </div>
                    <hr />

                    <div class="row" id="method" style="max-width:1000px; margin:0 auto; text-align:justify">
                        <center>
                            <img src="./data/imgs/collect_real_2.png" width="100%">
                        </center>
                        <br>
                        <p style="text-align: justify; font-size: medium;">
                            <b>VinT-Real</b> : Data collection mirroring toddler-like exploration.
                        </p>
                        <br>
                    </div>
                    <div align="center">
                        <h1>VinT-Net Framework</h1>
                    </div>
                    <div class="row" id="network" style="max-width:1000px; margin:0 auto; text-align:justify">
                        <center>
                            <img src="./data/imgs/network_framework.png" width="100%">
                        </center>
                        <br>
                        <p style="text-align: justify; font-size: medium;">
                            We introduce VinT-Net to enhance research in robotic object-
                            in-hand pose estimation and set up standard benchmarks.
                            This network acts as a simple yet effective baseline for multi-
                            finger object-in-hand pose estimation challenges. VinT-
                            Net can efficiently process various inputs, such as RGB
                            and depth images with segmentation labels and local touch
                            points derived from robotic tactile and proprioceptive sen-
                            sors. The architecture of VinT-Net comprises two crucial
                            sub-modules: a sensing aggregation module and a 3D keypoint-based pose estimation module. The framework of VinT-Net is illustrated
                            in Figure 8. It accurately estimates the object’s 6D pose
                            from the robot’s camera perspective
                        </p>
                        <br>
                    </div>

                    <div align="center"><h1>BibTeX</h1></div>
                    <hr />
                    <pre style="font-size: medium;">
@InProceedings{vint6d,
author       = {Zhaoliang Wan and Yonggen Ling and Senlin Yi and Lu Qi and Minglei Lu and Wangwei Lee and Sicheng Yang and Xiao Teng and Peng Lu and Xu Yang and Ming-Hsuan Yang and Hui Cheng},
title        = {VinT-6D: A Large-Scale Object-in-hand Dataset from Vision, Touch and Proprioception },
booktitle    = {Forty-First International Conference on Machine Learning},
year         = {2024},
}
                    </pre>

                </div>
            </div>
        </div>
        <br/><br/>

        <script
            src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
            integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
            crossorigin="anonymous"
        ></script>
        <script
            src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
            integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
            crossorigin="anonymous"
        ></script>
        <script
            src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
            integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
            crossorigin="anonymous"
        ></script>
    </body>
</html>
